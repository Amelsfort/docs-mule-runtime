= DataWeave Output Formats and Writer Properties
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]
:keywords: studio, anypoint, esb, transform, transformer, format, aggregate, rename, split, filter convert, xml, json, csv, pojo, java object, metadata, dataweave, data weave, datamapper, dwl, dfl, dw, output structure, input structure, map, mapping

DataWeave can read and write many types of data formats, such as JSON, XML, and
many others.
//LINK TO DW 1.0 LANDING PAGE IN MULE 3.9 DOCS:
include::partial$dataweave1-links.adoc[tag=dataweave1LandingPage]

DataWeave supports these formats (or MIME types) as input and output:

[cols="2,2,2", options="header"]
|===
| MIME Type | ID | Supported Formats

| `application/avro`
| `avro`
| <<format_avro>>

| `application/csv`
| `csv`
| <<format_csv>>

| `application/dw`
| `dw`
| <<format_dataweave>> (for testing a DataWeave expression)

| `application/flatfile`
| `flatfile`
| <<format_flat_file>>, <<format_cobol_copybook>>, <<format_fixed_width>>

| `application/java`
| `java`
| <<format_java>>, <<format_enum>>

| `application/json`
| `json`
| <<format_json>>

| `application/octet-stream`
| `binary`
| <<format_octet_stream>> (for binaries)

| `application/yaml`
| `yaml`
| <<format_yaml>>

| `application/xml`
| `xml`
| <<format_xml>>, <<format_cdata>>

| `application/x-ndjson`
| `ndjson`
| <<format_ndjson>> (Newline Delimited JSON)

| `application/xlsx`
| `excel`
| <<format_excel>>

| `application/x-www-form-urlencoded`
| `urlencoded`
| <<format_url_encoded>>

| `multipart/*`
| `multipart`
| <<format_form_data>>

| `text/plain`
| `text`
| <<format_text_plain>> (for plain text)

| `text/x-java-properties`
| `properties`
| <<format_x_java_properties>> (Properties)
|===
// TODO: PLAIN TEXT SECTION? <<format_plain_text>>

[[dw_readers_writers]]
== DataWeave Readers and Writers

DataWeave can read input data as a whole in-memory, in indexed fashion,
and for some data formats, part-by-part by streaming the input. When attempting
to read a large file, it is possible to run out of memory or to impact
performance negatively. Streaming can improve performance but impacts access to file.

* Indexed and In-Memory: Allow for random access to data because both
strategies parse the entire document. When using these strategies, a DataWeave
script can access any part of the resulting value at any time.
** Indexed: Uses indexes over the disk.
** In-Memory: Parses the entire document in memory.

* Streaming: Allows for sequential access to the file. This strategy partitions
the input document into smaller items and accesses its data sequentially,
storing the current item in memory. A DataWeave selector can access the portion
of the file that is getting read. DataWeave supports streaming for a few
formats:
** <<format_csv, CSV>>
** <<format_json, JSON>>
** <<format_xml, XML>> (starting in Mule 4.3.0)
** <<format_excel, XLSX>> (starting in Mule 4.2.2)

[[reader_writer_properties]]
=== Using Reader and Writer Properties

In some cases, it is necessary to modify or specify aspects of the format
through format-specific properties. For example, you can specify CSV input and
output properties, such as the `separator` (or delimiter) to use in the CSV file.
For Cobol copybook, you need to specify the path to a schema file using the
`schemaPath` property.

You can append reader properties to the MIME type (`outputMimeType`) attribute
for certain components in your Mule app. Listeners and Read operations accept
these settings. For example, this On New File listener example identifies the `,` separator for a CSV input file:

.Example: Properties for the CSV Reader
[source,xml,linenums]
----
<file:listener doc:name="On New File" config-ref="File_Config" outputMimeType='application/csv; separator=","'>
  <scheduling-strategy >
    <fixed-frequency frequency="45" timeUnit="SECONDS"/>
  </scheduling-strategy>
  <file:matcher filenamePattern="comma_separated.csv" />
</file:listener>
----

Note that the `outputMimeType` setting above helps the CSV _reader_ interpret
the format and delimiter of the input `comma_separated.csv` file, not the writer.

To specify the output format, you can provide the MIME type and any writer
properties for the writer, such as the CSV or JSON writer used by a File Write
operation. For example, you might need to write a pipe (`|`) delimiter in your
CSV output payload, instead of some other delimiter used in the input. To do
this, you append the property and its value to the `output` directive of a
DataWeave expression. For example, this Write operation specifies the pipe as
a `separator`:

.Example: output Directive for the CSV Writer
[source,xml,linenums]
----
<file:write doc:name="Write" config-ref="File_Config" path="my_transform">
  <file:content ><![CDATA[#[output application/csv separator="|" --- payload]]]></file:content>
</file:write>
----

The sections below list the format-specific reader and writer properties
available for each supported format.

[[set_mime_types]]
== Setting MIME Types

You can specify the MIME type for the input and output data that flows through
a Mule app.

For DataWeave transformations, you can specify the MIME type for the output data.
For example, you might set the `output` header directive of an expression in the
Transform Message component or a Write operation to `output application/json` or
`output application/csv`.

Starting in Mule 4.3.0, you can set the `output` directive using the format ID alone,
instead of using the MIME type. For example, you might set the `output` header directive
of an expression in the Transform Message component or a Write operation to
`output json` or `output csv`. You can also use the format ID to
differentiate the format of the output data from the MIME type in the output header.
For example, you can write JSON data but customize the MIME type to
 `application/problem+json` by using the DataWeave directive `output application/problem+json with json`.
 See xref:dataweave-cookbook-change-script-output-mime.adoc[Change a Script's MIME Type Output]
 for examples that customize the MIME type output of a script.

This example sets the MIME type through a File Write operation to ensure that a
format-specific writer, the CSV writer, outputs the payload in CSV format:

.Example: MIME Type for the CSV Writer
[source,xml,linenums]
----
<file:write doc:name="Write" config-ref="File_Config" path="my_transform">
  <file:content ><![CDATA[#[output application/csv --- payload]]]></file:content>
</file:write>
----

For input data, format-specific readers for Mule sources (such as the
On New File listener), Mule operations (such as Read and HTTP Request
operations), and DataWeave expressions attempt to infer the MIME type
from metadata that is associated with input payloads, attributes, and
variables in the Mule event. When the MIME type cannot be inferred from
the metadata (and when that metadata is not static), Mule sources and
operations allow you to specify the MIME type for the reader. For example,
you might set the MIME type for the On New File listener to
`outputMimeType='application/csv'` for CSV file _input_. This setting provides
information about the file format to the CSV reader.

.Example: MIME Type for the CSV Reader
[source,xml,linenums]
----
<file:listener doc:name="On New File"
  config-ref="File_Config"
  outputMimeType='application/csv'>
</file:listener>
----

Note that reader settings _are not_ used to perform a transformation from one
format to another. They simply help the reader interpret the format of the input.

You can also set special reader and writer properties for use by the
format-specific reader or writer of a source, operation, or component.
See <<reader_writer_properties>>.

////////////////////////////////////////////////////////////////////////////
// application/avro ////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////
[[format_avro]]
== Avro

MIME type: `application/avro`

ID: `avro`

Avro is a binary data format that uses a schema to structure its data. DataWeave relies on the schema to parse the data. Avro data structures are mapped to DataWeave data structures.

[[java_value_mapping_avro]]
=== Java Value Mapping

The following table shows how Avro types map to DataWeave types.

[cols="2,2",options="header"]
|===
| Avro Type | DataWeave Type
| `long`| Number
| `int`| Number
| `double`| Number
| `boolean`| Boolean
| `string`| String
| `fixed`| String
| `bytes`| Binary
| `enum`| String
| `map`| Object
| `array`| Array
| `null`| Null
|===

[[example1_avro]]
=== Example

The following example shows how to use a schema to output an Avro data structure.

==== Input

An Avro schema looks something like this.

.schema.json:
[source,json,linenums]
----
{
    "type": "record",
    "name": "userInfo",
    "namespace": "my.example",
    "fields": [
        {
            "name": "username",
            "type": "string",
            "default": "NONE"
        },
        {
            "name": "age",
            "type": "int",
            "default": -1
        },
        {
            "name": "phone",
            "type": "string",
            "default": "NONE"
        },
        {
            "name": "housenum",
            "type": "string",
            "default": "NONE"
        }
    ]
}
----

===== Source

The `schemaUrl` property in the header of this DataWeave script passes a schema (`schema.json`) to the DataWeave writer. The writer uses the schema to structure content from the body of the script and output the results in Avro format.

[source,weave,linenums]
----
%dw 2.0
output application/avro schemaUrl="classpath://schema.json"
---
[{
    username: "Mariano",
    age: 35,
    phone: "213",
    housenum: "123"
},
{
    username: "Leandro",
    age: 29,
    phone: "213",
    housenum: "123"
},
{
    username: "Christian",
    age: 25,
    phone: "213",
    housenum: "123"
}]
----

[[properties_avro]]
=== Configuration Properties

DataWeave supports the following configuration properties for Avro.

==== Reader Properties (for Avro)

DataWeave accepts optional parameters that provide instructions for reading input data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `schemaUrl` | `String` | None | The URL for the Avro schema. Valid values are `classpath://`, `file://`, or `http://`.
|===

==== Writer Properties (for Avro)

DataWeave accepts optional parameters that provide instructions for writing  output data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
  Valid values are `true` or `false`.
| `schemaUrl` | `String` | None | The URL for Avro schema. Valid values are `classpath://`, `file://`, or `http://`.
|===

[[mime_types_avro]]
=== Supported MIME Types (for Avro)

DataWeave supports the following MIME type.

[cols="1", options="header"]
|===
| MIME Type
|`application/avro`
|===

////////////////////////////////////////////////////////////////////////////
// application/flatfile ////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////
[[format_cobol_copybook]]
== Cobol Copybook

MIME Type: `application/flatfile`

ID: `flatfile`

A Cobol copybook is a type of flat file that describes the layout of records and fields in a Cobol data file.

The Transform Message component provides settings for handling the Cobol copybook format. For example, you can import a Cobol definition into the Transform Message component and use it for your Copybook transformations.

NOTE: Cobol copybook in DataWeave supports files of up to 15 MB, and the memory requirement is roughly 40 to 1. For example, a 1-MB file requires up to 40 MB of memory to process, so it's important to consider this memory requirement in conjunction with your TPS needs for large copybook files. This is not an exact figure; the value might vary according to the complexity of the mapping instructions.


[[cobol_metadata]]
=== Importing a Copybook Definition

When you import a Copybook definition, the Transform Message component converts the definition to a flat file schema that you can reference with `schemaPath` property.

To import a copybook definition:

. Right-click the input payload in the Transform component in Studio, and select *Set Metadata* to open the Set Metadata Type dialog.
+
Note that you need to create a metadata type before you can import a copybook
definition.
+
. Provide a name for your copybook metadata, such as `copybook`.
. Select the Copybook type from the *Type* drop-down menu.
. Import your copybook definition file.
. Click Select.
+
.Importing a Copybook Definition File
image::copybook-import.png[Importing a Copybook Definition]

For example, assume that you have a copybook definition file
(`mailing-record.cpy`) that looks like this:

----
       01  MAILING-RECORD.
           05  COMPANY-NAME            PIC X(30).
           05  CONTACTS.
               10  PRESIDENT.
                   15  LAST-NAME       PIC X(15).
                   15  FIRST-NAME      PIC X(8).
               10  VP-MARKETING.
                   15  LAST-NAME       PIC X(15).
                   15  FIRST-NAME      PIC X(8).
               10  ALTERNATE-CONTACT.
                   15  TITLE           PIC X(10).
                   15  LAST-NAME       PIC X(15).
                   15  FIRST-NAME      PIC X(8).
           05  ADDRESS                 PIC X(15).
           05  CITY                    PIC X(15).
           05  STATE                   PIC XX.
           05  ZIP                     PIC 9(5).
----

* Copybook definitions must always begin with a `01` entry. A separate record
  type is generated for each `01` definition in your copybook (there must be at
  least one `01` definition for the copybook to be usable, so add one using an
  arbitrary name at the start of the copybook if none is present). If there are
  multiple `01` definitions in the copybook file, you can select which
  definition to use in the transform from the dropdown list.
* COBOL format requires definitions to only use columns 7-72 of each line. Data
  in columns 1-5 and past column 72 is ignored by the import process. Column 6
  is a line continuation marker.

When you import the schema, the Transform component converts the copybook file
to a flat file schema that it stores in the `src/main/resources/schema` folder
of your Mule project. In flat file format, the copybook definition above looks
like this:

----
form: COPYBOOK
id: 'MAILING-RECORD'
values:
- { name: 'COMPANY-NAME', type: String, length: 30 }
- name: 'CONTACTS'
  values:
  - name: 'PRESIDENT'
    values:
    - { name: 'LAST-NAME', type: String, length: 15 }
    - { name: 'FIRST-NAME', type: String, length: 8 }
  - name: 'VP-MARKETING'
    values:
    - { name: 'LAST-NAME', type: String, length: 15 }
    - { name: 'FIRST-NAME', type: String, length: 8 }
  - name: 'ALTERNATE-CONTACT'
    values:
    - { name: 'TITLE', type: String, length: 10 }
    - { name: 'LAST-NAME', type: String, length: 15 }
    - { name: 'FIRST-NAME', type: String, length: 8 }
- { name: 'ADDRESS', type: String, length: 15 }
- { name: 'CITY', type: String, length: 15 }
- { name: 'STATE', type: String, length: 2 }
- { name: 'ZIP', type: Integer, length: 5, format: { justify: ZEROES, sign: UNSIGNED } }
----

After importing the copybook, you can use the `schemaPath` property to reference the associated flat file through the `output` directive. For example:
`output application/flatfile schemaPath="src/main/resources/schemas/mailing-record.ffd"`

=== Supported Copybook Features

Not all copybook features are supported by the Cobol Copybook format in
DataWeave. In general, the format supports most common usages and simple
patterns, including:

* USAGE of DISPLAY, BINARY (COMP), COMP-5, and PACKED-DECIMAL (COMP-3).
For character encoding restrictions, see <<character_encoding>>.
* PICTURE clauses for numeric values consisting only of:
** '9' - One or more numeric character positions
** 'S' - One optional sign character position, leading or trailing
** 'V' - One optional decimal point
** 'P' - One or more decimal scaling positions
* PICTURE clauses for alphanumeric values consisting only of 'X' character positions
* Repetition counts for '9', 'P', and 'X' characters in PICTURE clauses
  (as in `9(5)` for a 5-digit numeric value)
* OCCURS DEPENDING ON with controlVal property in schema. Note that if the
  control value is nested inside a containing structure, you need to manually
  modify the generated schema to specify the full path for the value in the
  form "container.value".
* REDEFINES clause (used to provide different views of the same portion of
  record data - see details in section below)

_Unsupported_ features include:

* Alphanumeric-edited PICTURE clauses
* Numeric-edited PICTURE clauses, including all forms of insertion, replacement, and zero suppression
* Special level-numbers:
** Level 66 - Alternate name for field or group
** Level 77 - Independent data item
** Level 88 - Condition names (equivalent to an enumeration of values)
* SIGN clause at group level (only supported on elementary items with PICTURE clause)
* USAGE of COMP-1 or COMP-2 and of clause at group level (only supported on elementary items with PICTURE clause)
* VALUE clause (used to define a value of a data item or conditional name from
  a literal or another data item)
* SYNC clause (used to align values within a record)

=== REDEFINES Support

REDEFINES facilitates dynamic interpretation of data in a record. When you import a
copybook with REDEFINES present, the generated schema uses a special grouping
with the name '*' (or '*1', '*2', and so on, if multiple REDEFINES groupings are present
at the same level) to combine all the different interpretations. You use this
special grouping name in your DataWeave expressions just as you use any other
grouping name.

Use of REDEFINES groupings has higher overhead than normal copybook groupings,
so MuleSoft recommends that you remove REDEFINES from your copybooks where possible
before you import them into Studio.

[[character_encoding]]
=== Character Encodings

BINARY (COMP), COMP-5, or PACKED-DECIMAL (COMP-3) usages are only supported
with single-byte character encodings, which use the entire range of 256 potential
character codes. UTF-8 and other variable-length encodings are not supported for
these usages (because they're not single-byte), and ASCII is also not supported
(because it doesn't use the entire range). Supported character encodings include
ISO-8859-1 (an extension of ASCII to full 8 bits) and other 8859 variations and
EBCDIC (IBM037).

REDEFINES requires you to use a single-byte-per-character character encoding for
the data, but any single-byte-per-character encoding can be used unless BINARY
(COMP), COMP-5, or PACKED-DECIMAL (COMP-3) usages are included in the data.

=== Common Copybook Import Issues

The most common issue with copybook imports is a failure to follow the Cobol
standard for input line regions. The copybook import parsing ignores the
contents of columns 1-6 of each line, and ignores all lines with an '*'
(asterisk) in column 7. It also ignores everything beyond column 72 in each line.
This means that all your actual data definitions need to be within columns 8
through 72 of input lines.

Tabs in the input are not expanded because there is no defined standard for tab
positions. Each tab character is treated as a single space character when
counting copybook input columns.

Indentation is ignored when processing the copybook, with only level-numbers
treated as significant. This is not normally a problem, but it means that
copybooks might be accepted for import even though they are not accepted by
Cobol compilers.

Both warnings and errors might be reported as a result of a copybook import.
Warnings generally tell of unsupported or unrecognized features, which might or
might not be significant. Errors are notifications of a problem that means the
generated schema (if any) will not be a completely accurate representation of
the copybook. You should review any warnings or errors reported and decide on
the appropriate handling, which might be simply accepting the schema as
generated, modifying the input copybook, or modifying the generated schema.

=== Reader Properties (for Cobol Copybook)

DataWeave accepts optional parameters that provide instructions for reading input data. For details, see <<reader_properties_flat_file>>.

Note that schemas with type `Binary` or `Packed` don't allow for the detection
of line breaks, so setting `recordParsing` to `lenient` only allows for long
records to be handled, not short ones. These schemas only work with certain
single-byte character encodings (so not with UTF-8 or any multibyte format).

=== Writer Properties (for Cobol Copybook)

DataWeave accepts optional parameters that provide instructions for writing output data. For details, see <<writer_properties_flat_file>>.

.Example: output Directive
[source,dataweave,linenums]
----
output application/flatfile schemaPath="src/main/resources/schemas/QBReqRsp.esl", structureIdent="QBResponse"
----

////////////////////////////////////////////////////////////////////////////
// application/csv /////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////
[[format_csv]]
//DONE
== CSV

MIME Type: `application/csv`

ID: `csv`

The CSV data format is represented as a DataWeave array of objects in which each object represents a row. All simple values are represented in a DataWeave string.

To understand the parsing strategies that DataWeave readers and writers can apply to this format, see xref:#dw_readers_writers[DataWeave Parsing Strategies].

=== Examples

The following examples show uses of the CSV format.

[[example1_csv]]
===== Example: Representing CSV Data

The following example shows how DataWeave represents CSV data.

===== Input

The following sample data serves as input for the DataWeave script.

[source,csv,linenums]
----
name,lastname,age,gender
Mariano,de Achaval,37,male
Paula,de Estrada,37,female
----

===== Source

The DataWeave script transforms the CSV input payload to the DataWeave (dw) format and MIME type.

[source,dataweave,linenums]
----
%dw 2.0
output application/dw
---
payload
----

===== Output

The DataWeave script produces the following output.

[source,dataweave,linenums]
----
[
  {
    name: "Mariano",
    lastname: "de Achaval",
    age: "37",
    gender: "male"
  },
  {
    name: "Paula",
    lastname: "de Estrada",
    age: "37",
    gender: "female"
  }
]
----

[[example2_csv]]
=== Example: Streaming CSV Data

By default, the CSV reader stores input data from an entire file in-memory
if the file is 1.5MB or less. If the file is larger than 1.5 MB, the process
writes the data to disk. For very large files, you can improve the performance
of the reader by setting a `streaming` property to `true`. To demonstrate the use of this property, the next example streams a CSV file and transforms it to JSON.

===== Input

The structure of the CSV input looks something like the following. Note that a streamed file is typically much longer.

.CSV File Input for Streaming Example (truncated):
[source,csv,linenums]
----
street,city,zip,state,beds,baths,sale_date
3526 HIGH ST,SACRAMENTO,95838,CA,2,1,Wed May 21 00:00:00 EDT 2018
51 OMAHA CT,SACRAMENTO,95823,CA,3,1,Wed May 21 00:00:00 EDT 2018
2796 BRANCH ST,SACRAMENTO,95815,CA,2,1,Wed May 21 00:00:00 EDT 2018
2805 JANETTE WAY,SACRAMENTO,95815,CA,2,1,Wed May 21 00:00:00 EDT 2018
6001 MCMAHON DR,SACRAMENTO,95824,CA,2,1,,Wed May 21 00:00:00 EDT 2018
5828 PEPPERMILL CT,SACRAMENTO,95841,CA,3,1,Wed May 21 00:00:00 EDT 2018
----

===== Source

To demonstrate a use of the `streaming` property, the following DataWeave script shows a Mule flow that streams a CSV file and transforms it to JSON.

[source,xml,linenums]
----
<flow name="dw-streamingFlow" >
  <scheduler doc:name="Scheduler" >
    <scheduling-strategy >
      <fixed-frequency frequency="1" timeUnit="MINUTES"/>
    </scheduling-strategy>
  </scheduler>
  <file:read
     path="${app.home}/input.csv"
     config-ref="File_Config"
     outputMimeType="application/csv; streaming=true; header=true"/>
  <ee:transform doc:name="Transform Message" >
    <ee:message >
      <ee:set-payload ><![CDATA[%dw 2.0
output application/json
---
payload map ((row) -> {
zipcode: row.zip
})]]></ee:set-payload>
    </ee:message>
  </ee:transform>
  <file:write doc:name="Write"
    config-ref="File_Config1"
    path="/path/to/output/file/output.json"/>
  <logger level="INFO" doc:name="Logger" message="#[payload]"/>
</flow>
----

* The example configures the Read operation (`<file:read/>`) to stream the CSV input by setting `outputMimeType="application/csv; streaming=true"`. The input CSV file is located in the project directory, `src/main/resources`, which is the location of `${app.home}`.
* The DataWeave script in the *Transform Message* component uses the `map`
function to iterate over each row in the CSV payload and select the value
of each field in the `zip` column.
* The Write operation returns a file, `output.json`, which contains the result
of the transformation.
* The Logger prints the same output payload that you see in `output.json`.

===== Output

The CSV streaming example produces the following output.

[source,json,linenums]
----
[
  {
    "zipcode": "95838"
  },
  {
    "zipcode": "95823"
  },
  {
    "zipcode": "95815"
  },
  {
    "zipcode": "95815"
  },
  {
    "zipcode": "95824"
  },
  {
    "zipcode": "95841"
  }
]
----

[[properties_csv]]
=== Configuration Properties

DataWeave supports the following configuration properties for CSV.

=== Reader Properties (for CSV)

DataWeave accepts optional parameters that provide instructions for reading input data.

//TODO: CHECK STREAMING ENTRIES

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `bodyStartLineNumber` | `Number` | `0` | The line number on which the body starts.
| `escape` | `String` | `\` | Character used to escape invalid characters, such as separators or quotes within field values.
| `header` | `Boolean` |`true` | Indicates whether a CSV header is present.
 Valid values are `true` or `false`.

* If `header=true`, you can access the fields within the input
 by name, for example, `payload.userName`.
* If `header=false`, you must access the fields by index, referencing
 the entry first and the field next, for example, `payload[107][2]`.
| `headerLineNumber` | `Number` | `0` | The line number on which the CSV header is located.
| `ignoreEmptyLine`| `Boolean` | `true` | Ignores any empty line.
Valid values are `true` or `false`.
| `quote` | `String` | `"` | Character to use for quotes.
| `separator` | `String` | `,` | Character that separates one record from another.
| `streaming` | `Boolean` | `false` | Property for streaming CSV input. Use only if entries are accessed sequentially. Valid values are `true` or `false`.
See the <<example2_csv, streaming example>>, and see <<dw_readers_writers>>.
|===

[[writer_properties_csv]]
=== Writer Properties (for CSV)

DataWeave accepts optional parameters that provide instructions for writing output data.

A CSV output directive example might look like this:

.Example: output Directive
[source,dataweave,linenums]
----
output application/csv separator=";", header=false, quoteValues=true
----

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `bodyStartLineNumber` | `Number` | `0` | Line number on which the body starts.
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
  Valid values are `true` or `false`.
| `encoding` | `String` | None | Encoding for the writer to use, such as `UTF-8`.
| `escape` | `String` | `\` | Character to use for escaping an invalid character,
such as occurrences of the separator or quotes within field values.
| `header` | `Boolean` | `true` | Indicates whether to write a CSV header. Valid values are `true` or `false`.
| `headerLineNumber` | `Number` | `0` | Identifies the line number on which the header is located.
| `ignoreEmptyLine` | `Boolean` | `true` | Ignores any empty line.
Valid values are `true` or `false`.
| `lineSeparator` | `String` | New Line | Line separator to use when writing the CSV, for example, `\r\n`.
| `quote` | `String` | `"` | The character to be used for quotes.
| `quoteHeader` | `Boolean` | `false` | Indicates whether to quote header values.
Valid values are `true` or `false`.
| `quoteValues` | `Boolean` | `false` | Indicates whether to quote every value
(even if the value contains special characters). Valid values are `true` or `false`.
| `separator` | `String` | `,` | Character that separates records from another.
+
//TODO: no streaming property for writers, right? Commented out for now.
+
//| `streaming` | `Boolean` | `false` | Property for streaming input. Use only if entries are accessed sequentially. Valid values are `true` or `false`. For more on streaming in DataWeave, see <<dw_readers_writers>>.
+
|===

[[mime_types_csv]]
=== Supported MIME Types (for CSV)

DataWeave supports the following MIME type.

[cols="1", options="header"]
|===
| MIME Type
|`*/csv`
|===

////////////////////////////////////////////////////////////////////////////
// application/dw //////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////
[[format_dataweave]]
== DataWeave (dw)

MIME Type: `application/dw`

ID: `dw`

//DONE: dw overview and examples
The DataWeave (dw) format is the canonical format for all transformations.
This format can help you understand how input data is interpreted before it is
transformed to a new format.

[NOTE]
====
This format is intended to help you debug the results of DataWeave transformations. It is significantly slower than other formats. It is not recommended to be used in production applications because it can impact the performance.
====

[[examples_dw]]
=== Example

The following example shows how XML input is expressed in the DataWeave format.

==== Input

The example uses the following XML snippet as input.

[source,xml,linenums]
----
<employees>
  <employee>
    <firstname>Mariano</firstname>
    <lastname>DeAchaval</lastname>
  </employee>
  <employee>
    <firstname>Leandro</firstname>
    <lastname>Shokida</lastname>
  </employee>
</employees>
----

===== Output

.Output: in DataWeave Format
[source,dataweave,linenums]
----
{
  employees: {
    employee: {
      firstname: "Mariano",
      lastname: "DeAchaval"
    },
    employee: {
      firstname: "Leandro",
      lastname: "Shokida"
    }
  }
} as Object {encoding: "UTF-8", mimeType: "text/xml"}
----

//DONE: properties_dw
[[properties_dw]]
=== Configuration Properties

DataWeave supports the following configuration properties for the DataWeave (dw) format.

==== Reader Properties (for dw)

DataWeave accepts optional parameters that provide instructions for reading input data.

[cols="2,1,1,2", options="header"]
|===
|Parameter |Type |Default|Description
| `externalResources` | `Boolean` | `false` | Enables reading external entities on `readUrl`. Valid options are `true` or `false`.
| `javaModule`| `Boolean` |`false`| Enables you to load Java module functions. Valid options are `true` or `false`.
| `onlyData` | `Boolean` | `true` | Indicates whether to handle data only and not other types of content, such as functions. Valid  options are `true` or `false`. Note that the parser runs faster in the `onlyData` mode.
|===

==== Writer Properties (for dw)

DataWeave accepts optional parameters that provide instructions for writing output data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
  Valid values are `true` or `false`.
| `ignoreSchema` | `Boolean` | `false` | Flag that indicates whether the DataWeave writer
ignores the schema. Valid values are `true` or `false`.
| `indent` | `String` | `'  '` | The string to use for indentation.
| `maxCollectionSize` | `Number` | `-1` | The maximum number of elements allowed
in an array or object. `-1` indicates that no limitation is set.
|`onlyData` | `Boolean` | `true` | Indicates whether to handle data only and not other types of content, such as functions. Valid  options are `true` or `false`. Note that the parser runs faster in the `onlyData` mode.
|===

=== Supported MIME Types (for dw)

DataWeave supports the following MIME type.

[cols="1", options="header"]
|===
| MIME Type
|`*/dw`
|===

// application/xlsx //////////////////////////////////////////////////////////
[[format_excel]]
== Excel

MIME Type: `application/xlsx`

ID: `excel`

An Excel workbook is a sequence of sheets. In DataWeave, this is mapped to an
object where each sheet is a key. Only one table is allowed per Excel sheet.
A table is expressed as an array of rows. A row is an object where its keys
are the columns and the values the cell content.

[NOTE]
====
Only `.xlsx` files are supported (Excel 2007). `.xls` files are not supported
by Mule.
====

To understand the parsing strategies that DataWeave readers and writers can apply to this format, see xref:#dw_readers_writers[DataWeave Parsing Strategies].

[[dw_type_mapping_excel]]
=== Excel Type Mapping

The following table shows how Excel types map to DataWeave types.

[cols="2,2",options="header"]
|===
| Excel Type | DataWeave Type
| String| Number
| Numeric| Number
| Boolean| Boolean
| Data| Date
|===


[[examples_excel]]
=== Examples

The following examples show uses of the Excel format.

[[example1_excel]]
==== Example

This example shows how DataWeave represents an Excel workbook.

===== Input

The Excel workbook (`Sheet1`) serves as an input payload for the example.

.Sheet1:
[cols="2,2,2",options="header"]
===
.   | A    | B
1   | Id   | Name
2   | 123  | George
3   | 456  | Lucas
===

===== Source

The DataWeave script transforms the Excel input payload to the DataWeave (dw) format and MIME type.

[source,dataweave,linenums]
----
%dw 2.0
output application/dw
---
payload
----

===== Output

The DataWeave output looks like this. You can select values the same way you select values in other objects.

[source,dataweave,linenums]
----
{
    "Sheet1": [
        {
                "A": 123,
                "B": "George"
        },
        {
                "A": 456,
                "B": "Lucas"
        }
    ]
}

----

[[example2_excel]]
==== Example

The following DataWeave script outputs an Excel table with the header and fields.

===== Source

The body of this DataWeave script is a DataWeave object that defines the content of the Excel sheet. The name of the sheet, `Sheet1`, is the key of this object. The value is an array of objects. Each object in the array contains a collection of key-value pairs. The keys in each pair are treated as header values for the
spreadsheet. The values in each pair are treated as data values for a row in the sheet.

The output directive indicates that the output is the Excel format and MIME type. The `header=true` setting indicates that the output includes the header values.

[source,dataweave,linenums]
----
%dw 2.0
output application/xlsx header=true
---
{
  Sheet1: [
    {
      Id: 123,
      Name: George
    },
    {
      Id: 456,
      Name: Lucas
    }
  ]
}
----

For another example, see
xref:dataweave-cookbook-xlsx-lookup.adoc[Look Up Data in an Excel (XLSX) File].

===== Example: Streaming Excel

By default, the Excel reader stores input data from an entire file in-memory
if the file is 1.5MB or less. If the file is larger than 1.5 MB, the process
writes the data to disk. For very large files, you can improve the performance
of the reader by setting a `streaming` property to `true`. To demonstrate the use of this property, the next example streams a XLSX file and transforms it to JSON.

The following example streams an Excel file and transforms it to JSON.

[source,xml,linenums]
----
<http:listener-config
    name="HTTP_Listener_config"
    doc:name="HTTP Listener config" >
  <http:listener-connection host="0.0.0.0" port="8081" />
</http:listener-config>
<flow name="streaming_flow" >
  <http:listener
    doc:name="Listener"
    config-ref="HTTP_Listener_config"
    path="/"
    outputMimeType="application/xlsx; streaming=true"/>
  <ee:transform doc:name="Transform Message" >
    <ee:message >
      <ee:set-payload ><![CDATA[%dw 2.0
output application/json
---
payload."Sheet Name" map ((row) -> {
    foo: row.a,
    bar: row.b
})]]></ee:set-payload>
    </ee:message>
  </ee:transform>
</flow>
----

The example:

* Configures the HTTP listener to stream the XLSX input
by setting `outputMimeType="application/xlsx; streaming=true"`.
In the Studio UI, you can use the *MIME Type* on the listener to `application/xlsx`
and the *Parameters* for the MIME Type to *Key* `streaming` and *Value* `true`.
* Uses a DataWeave script in the *Transform Message* component to iterate
over each row in the XLSX payload (an XLSX sheet called `"Sheet Name"`) and
select the values of each cell in the row (using `row.a`, `row.b`). It assumes
columns named `a` and `b` and maps the values from each row in those columns
into `foo` and `bar`, respectively.

===== Output

The following image shows the Excel table output.

image::dataweave-formats-exceltable.png[]

[[properties_excel]
=== Configuration Properties

DataWeave supports the following configuration properties for Excel.

==== Reader Properties (for Excel)

DataWeave accepts optional parameters that provide instructions for reading input data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `header` | `Boolean` | `true` |	Indicates whether the Excel table contains
headers. Valid values are `true` or `false`.
| `ignoreEmptyLine` | `Boolean` | `true` | Indicates whether to ignore empty
line. Valid values are `true` or `false`.
| `streaming` | `Boolean` | `false` | Introduced in Mule 4.2.2: Streaming is intended for processing a large file. When streaming is enabled, the reader accesses each row sequentially, keeping one row in memory at a time, instead of making all data available at once. Streaming does not permit random access to rows in the file. Use only if the entries are accessed sequentially. Valid values are `true` or `false`.
| `tableOffset` | `String` | None | The position of the first cell in the
table (`<Column><Row> example A1`).
| `zipBombCheck` | `Boolean` | `true` | If set to `false`, the zip bomb check is turned off. Valid values are `true` or `false`.
|===

==== Writer Properties (for Excel)

DataWeave accepts optional parameters that provide instructions for writing output data.

[cols="1,1,1,3a", options="header"]
|===
|Parameter | Type | Default | Description
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
  Valid values are `true` or `false`.
| `header` | `Boolean` | `true` |	Indicates whether the Excel table contains
headers. Valid values are `true` or `false`. When there are no headers, column names are used (for example, A, B, C, and so on).
| `ignoreEmptyLine` | `Boolean` | `true` | Indicates whether to ignore empty
lines. Valid values are `true` or `false`.
| `streaming` | `Boolean` | `false` | Introduced in Mule 4.2.2: Streaming is intended for processing a large file. Use only if the entries are accessed sequentially. Valid values are `true` or `false`.
| `tableOffset` | `String` | `None` | The position of the first cell in the table (`<Column><Row> example A1`).
| `zipBombCheck` | `Boolean` | `true` | If set to `false`, the zip bomb check is turned off. Valid values are `true` or `false`.
|===

=== Supported MIME Types (for Excel)

DataWeave supports the following MIME type.

[cols="1", options="header"]
|===
| MIME Type
|`application/vnd.openxmlformats-officedocument.spreadsheetml.sheet`
|`application/xlsx`
|===

///////////////////////////////////////////////////////////////////////////////
// application/flatfile ///////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
[[format_fixed_width]]
== Fixed Width

MIME Type: `application/flatfile`

ID: `flatfile`

Fixed width types are technically considered a type of Flat File format, but
when selecting this option, the Transform component offers you settings that are
better tailored to the needs of this format.

NOTE: Fixed width in DataWeave supports files of up to 15 MB, and the memory requirement is roughly 40 to 1. For example, a 1-MB file requires up to 40 MB of memory to process, so it's important to consider this memory requirement in conjunction with your TPS needs for large fixed width files. This is not an exact figure; the value might vary according to the complexity of the mapping instructions.

[[properties_fixed_width]]
=== Configuration Properties

DataWeave supports the following configuration properties for the Fixed Width format.

==== Reader Properties (for Fixed Width)

DataWeave accepts optional parameters that provide instructions for reading input data. For details, see <<reader_properties_flat_file>>.

Note that schemas with type `Binary` or `Packed` do not allow for the detection
of line breaks, so setting `recordParsing` to `lenient` only allows for long
records to be handled, not short ones. These schemas only work with certain
single-byte character encodings (so not with UTF-8 or any multibyte format).

=== Writer Properties (for Fixed Width)

DataWeave accepts optional parameters that provide instructions for writing output data. For details. see <<writer_properties_flat_file>>. All of the properties are optional.

A DataWeave output directive might look like this:

.Example: output Directive
[source,text,linenums]
----
output application/flatfile schemaPath="src/main/resources/schemas/payment.ffd", encoding="UTF-8"
----

//TODO: VERIFY Flat File supports Fixed Width
=== Supported MIME Types

The Fixed Width format supports the following MIME types.

[cols="1", options="header"]
|===
| MIME Type
|`*/flatfile`
|===

//TODO: DO WE WANT TO KEEP THIS SECTION?
=== Defining a Metadata Type (for Fixed Width)

In the Transform component, you can define a Fixed Width type through the
following methods:

* By providing a sample file.
* By pointing to a Flat File schema file.
* Through a graphical editor that allows you to set up each field manually.
+
image::dataweave-formats-27b3c.png[]
// TODO IS IMAGE OKAY?

///////////////////////////////////////////////////////////////////////////////
// application/flatfile ///////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
[[format_flat_file]]
== Flat File

MIME Type: `application/flatfile`

ID: `flatfile`

The Flat File format supports multiple types of fixed width records within a single message. The schema structure allows you to define how different record types are distinguished, and how the records are logically grouped.

NOTE: Flat File in DataWeave supports files of up to 15 MB, and the memory requirement is roughly 40 to 1. For example, a 1-MB file requires up to 40 MB of memory to process, so it's important to consider this memory requirement in conjunction with your TPS needs for large flat files. This is not an exact figure; the value might vary according to the complexity of the mapping instructions.

=== Examples

The following examples show uses of the Flat File format.

//NOT NEW BUT MOVED
==== Example

This example shows a DataWeave script that outputs data in the Flat File format.
Notice that it uses the `schemaPath` and `structureIndent` writer properties.

.DataWeave Script that Outputs a Flat File:
[source,dataweave,linenums]
----
%dw 2.0
output application/flatfile schemaPath="src/main/resources/test-data/QBReqRsp.esl", structureIdent="QBResponse"
---
payload
----

[[properties_flat_file]]
=== Configuration Properties

DataWeave supports the following configuration properties for the Flat File format.

[[reader_properties_flat_file]]
==== Reader Properties (for Flat File)

DataWeave accepts optional parameters that provide instructions for reading input data.

//NOTE: SOME VALUES HAVE TO BE PROVIDED MANUALLY
[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `enforceRequires` | `Boolean` | `false` | Error if required value missing.
  Valid values are `true` or `false`.
| `missingValues` | `String` | `nulls` for copybook schema, `spaces` otherwise | Fill character used to represent missing
values. To represent missing values in the input data, you can use:

* `none`: Treat all data as actual values
* `spaces`: Interpret a field consisting of only spaces as a missing value
* `zeroes`: Interpret numeric fields consisting of only '0' characters and
character fields consisting of only spaces as missing values
* `nulls`: Interpret a field consisting only of 0 bytes as a missing value

| `recordParsing` | `String` | `strict` |

Expected separation between lines/records:

* `strict`: line break expected at exact end of each record
* `lenient`: line break used but records may be shorter or longer than schema specifies
* `noTerminator`: records follow one another with no separation
* `singleRecord`: entire input is a single record

Note that schemas with type `Binary` or `Packed` don't allow for line break
detection, so setting `recordParsing` to `lenient` only allows long records
to be handled, not short ones. These schemas also currently only work with
certain single-byte character encodings
(so not with UTF-8 or any multibyte format).

| `schemaPath` | `String` | None | Schema definition. Location in your local disk of the schema file used to parse your input.
| `segmentIdent` | `String` | None | Segment identifier in the schema for fixed width or copybook schemas (only needed when parsing a single segment/record definition and if the schema includes multiple segment definitions).
| `structureIdent` | `String` | None | Structure identifier in schema for flatfile schemas (only needed when parsing a structure definition, and if the schema includes multiple structure definitions)
| `truncateDependingOn` | `Boolean` | `false` | Truncate COBOL
copybook DEPENDING ON values to length used.  Valid values are `true` or `false`.
| `zonedDecimalStrict` | `Boolean` | `false` | Use the `strict` ASCII form of
sign encoding for COBOL copybook zoned decimal values.
Valid values are `true` or `false`.
|===

[[writer_properties_flat_file]]
==== Writer Properties (for Flat File)

DataWeave accepts optional parameters that provide instructions for writing output data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
  Valid values are `true` or `false`.
| `encoding` | `String` | None | Encoding to be used by this writer,
such as `UTF-8`.
| `enforceRequires` | `Boolean` | `false` | Error if a required value is missing.
Valid values are `true` or `false`.
| `missingValues` | `String` | `nulls` for copybook schema, `spaces` otherwise | Fill character used to represent missing
values:

* `none`: Write nothing for missing values
* `spaces`: Fill field with spaces
* `zeroes`: Fill numeric fields with '0' characters and character fields with space characters
* `nulls`: Fill field with 0 bytes

| `recordTerminator` | `String` | System property `line.separator` | Record separator line break. Valid values:

* `lf`
* `cr`
* `crlf`
* None

Note that in Mule versions 4.0.4 and later, this is only used as a separator
when there are multiple records. Values translate directly to character codes
(`none` leaves no termination on each record).
| `schemaPath` | `String` | None | Schema definition. Path where the schema file
to be used is located.
| `segmentIdent` | `String` | None | Segment identifier in the schema for fixed width or copybook schemas (only needed when writing a single segment/record definition, and if the schema includes multiple segment definitions).
| `structureIdent` | `String` | None | Structure identifier in schema for flatfile schemas (only needed when writing a structure definition and if the schema includes multiple structure definitions)| `trimValues` | `Boolean` | `false` | Trim string values longer than the field length by truncating trailing characters. Valid values are `true` or `false`.
| `trimValues` | `Boolean` | `false` | Indicates whether trim values are longer than the field width. Valid Options are `true` or `false`.
| `truncateDependingOn` | `Boolean` | `false` | Truncate DEPENDING ON COBOL
copybook values to length used. Valid values are `true` or `false`.
| `zonedDecimalStrict` | `Boolean` | `false` | Use the `strict` ASCII form of
sign encoding for COBOL copybook zoned decimal values. Valid values are `true` or
`false`.
|===

=== Supported MIME Types (for Flat File)

DataWeave supports the following MIME type.

[cols="1", options="header"]
|===
| MIME Type
|`*/flatfile`
|===


///////////////////////////////////////////////////////////////////////////////// multipart/form-data ////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
[[format_form_data]]
== Multipart (Form-Data)

MIME Type: `multipart/form-data`

ID: `multipart`

DataWeave supports multipart subtypes, in particular `form-data`. These formats
enable you to handle several different data parts in a single payload,
regardless of the format each part has. To distinguish the beginning and end of
a part, a boundary is used and metadata for each part can be added through
headers.

[[examples_multipart]]
=== Examples

The following examples show uses of the Multipart format.

[[example1_multipart]]
==== Example

Within a DataWeave script, you can access and transform data from any of the
parts by selecting the `parts` element. Navigation can be array-based or key-based when parts feature a name to reference them by. The part's data can be
accessed through the `content` keyword, while headers can be accessed through
the `headers` keyword.

===== Input

This example serves as input to separate DataWeave scripts. shows a raw `multipart/form-data` payload with a `34b21` boundary consisting of 3 parts:

* a `text/plain` one named `text`
* an `application/json` file (`a.json`) named `file1`
* a `text/html` file (`a.html`) named `file2`

.Raw Multipart
[source,text,linenums]
----
--34b21
Content-Disposition: form-data; name="text"
Content-Type: text/plain

Book
--34b21
Content-Disposition: form-data; name="file1"; filename="a.json"
Content-Type: application/json

{
  "title": "Java 8 in Action",
  "author": "Mario Fusco",
  "year": 2014
}
--34b21
Content-Disposition: form-data; name="file2"; filename="a.html"
Content-Type: text/html

<!DOCTYPE html>
<title>
  Available for download!
</title>
--34b21--
----

===== Source

The following DataWeave script uses the raw `multipart/form-data` payload as input to produce `Book:a.json`.

.Reading Multipart Content:
[source,dataweave,linenums]
----
%dw 2.0
output text/plain
---
payload.parts.text.content ++ ':' ++ payload.parts[1].headers.'Content-Disposition'.filename
----

[[example2_multipart]]
==== Example

You can generate multipart content that DataWeave uses to build an object with a
list of parts, each containing its headers and content. The following
DataWeave script produces the raw multipart data (previously analyzed)
if the HTML data is available in the payload.

.Writing Multipart Content:
[source,dataweave,linenums]
----
%dw 2.0
output multipart/form-data
boundary='34b21'
---
{
  parts : {
    text : {
      headers : {
        "Content-Type": "text/plain"
      },
      content : "Book"
    },
    file1 : {
      headers : {
        "Content-Disposition" : {
            "name": "file1",
            "filename": "a.json"
        },
        "Content-Type" : "application/json"
      },
      content : {
        title: "Java 8 in Action",
        author: "Mario Fusco",
        year: 2014
      }
    },
    file2 : {
      headers : {
        "Content-Disposition" : {
            "filename": "a.html"
        },
        "Content-Type" : payload.^mimeType
      },
      content : payload
    }
  }
}
----

Notice that the key determines the part's name if the name is not explicitly
provided in the `Content-Disposition` header, and note that DataWeave can
handle content from supported formats, as well as references to unsupported
ones, such as HTML.

=== Reader Properties (for Multipart)

DataWeave accepts optional parameters that provide instructions for reading input data.

You can set the boundary for the reader to use when it analyzes the data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `boundary` | `String` | None | The multipart boundary value. A String to
delimit parts.
|===

Note that in the DataWeave `read` function, you can also pass the property as
an optional parameter. The scope of the property is limited to the DataWeave
script where you call the function.

=== Writer Properties (for Multipart)

DataWeave accepts optional parameters that provide instructions for writing output data.

.Example: output Directive
[source,dataweave,linenums]
----
output multipart/form-data
----

In the output directive, you can also set a property for the writer to use
when it outputs the data in the specified format.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `boundary` | `String` | None | The multipart boundary value. A String to
delimit parts.
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
  Valid values are `true` or `false`.
|===

For example, if a boundary is `34b21`, then you can pass this:

.Example: output Directive
[source,dataweave,linenums]
----
output multipart/form-data boundary=34b21
----

Note that in the DataWeave `write` function, you can also pass the property as
an optional parameter. The scope of the property is limited to the DataWeave
script where you call the function.

[TIP]
--
Multipart is typically, but not exclusively, used in HTTP where the boundary is
shared through the `Content-Type` header, both for reading and writing content.
--

// application/java //////////////////////////////////////////////////////////
[[format_java]]
//TODO: START AT JAVA
== Java

MIME Type: `application/java`

ID: `java`

TODO: Java Data Format will try to map any Java value to a DataWeave value.
This mapping is done in most cases by hand trying to map the semantics of both DataWeave and Java.

This table shows the mapping between Java objects to DataWeave types.

[cols="3,1", options="header"]
|===
|Java Type
|DataWeave Type

|`Collections/Array/Iterator/Iterable`
| xref:dataweave-types.adoc#array[Array]

|`String/CharSequence/Char/Enum/Class`
| xref:dataweave-types.adoc#string[String]

|`int/Short/Long/BigInteger/Flat/Double/BigDecimal`
|xref:dataweave-types.adoc#number[Number]

|`Calendar/XmlGregorianCalendar`
|xref:dataweave-types.adoc#datetime[DateTime]

|`TimeZone`
|xref:dataweave-types.adoc#timezone[TimeZone]

|`sql.Date/util.Date`
|xref:dataweave-types.adoc#date[Date]

|`Bean/Map`
|xref:dataweave-types.adoc#object[Object]

|`InputStream/Array[Byte]`
|xref:dataweave-types.adoc#binary[Binary]

|`java.lang.Boolean`
|xref:dataweave-types.adoc#boolean[Boolean]
|===


TODO: INTEGRATE TABLE BELOW???? TBD

[cols="2,2",options="header"]
|===
|Java Class | DataWeave
|java.lang.String  | String
|Enum| String
|Class| String
|UUID| String
|CharSequence| String
|Char| String
|java.sql.Clob| String
|java.io.Reader| String
|int | Number
|long| Number
|double| Number
|short| Number
|BigInteger| Number
|BigDecimal| Number
|AtomicInteger| Number
|AtomicLong| Number
|boolean| Boolean
|AtomicBoolean| Boolean
|java.util.Collection| Array
|java.lang.Iterable|Array
|java.util.Iterator|Array
|java.util.Map| Object
|java.util.Optional| Null or Value
|byte[]|Binary
|java.lang.InputStream|Binary
|byte|Binary
|java.lang.File|Binary
|java.time.LocalDateTime|LocalDateTime
|java.sql.Timestamp |LocalDateTime
|java.sql.Date  |LocalDateTime
|java.time.ZonedDateTime|DateTime
|java.time.LocalTime|LocalTime
|java.time.OffsetTime|Time
|java.time.LocalDate|Date
|java.time.ZoneOffset|TimeZone
|java.util.Calendar|DateTime
|java.util.XMLGregorianCalendar|DateTime
| *[] | Array
|===

TODO: IMPORTANT: If it is not in present in this table then is going to be handle as a `Java Bean` and it is mapped as a `Object` type where all properties are taken from the getters.

== Metadata

TODO

=== ^class metadata

TODO: ALL

All Java Object has a associated class.
To keep this information available in DataWeave it is mapped to a Metadata property.

`payload.^class` is going to return the name of the class of the `payload`.

Also this metadata property is used by the java writer to know to which java class a DataWeave value must be mapped to.
In other words what java class is going to be created from a given DataWeave value.

`output application/java --- now() as DateTime {class: "java.util.Calendar"}` this expression is going to create a `java.util.Calendar`

== Custom Types

TODO

=== Enum

TODO: In order to put an enum value in a java.util.Map, the DataWeave Java module defines a custom type called Enum.
It allows you to specify that a given string should be handled as the name of a specified enum type.
It should always be used with the `class` metadata property with specifing Java class name of the enum.
For example:

`{gender: "Female" as Enum {class: "org.mycompany.Gender"}}`

== Examples

TODO: This examples are going to use this class definitions:

[source,java,linenums]
----
class User {
    private String name;
    private String lastName;

    public User(){

    }

    public User(String name, String lastName){
        this.name = name;
        this.lastName = lastName;
    }

    public void setName(String name){
        this.name = name;
    }

     public void setLastName(String lastName){
        this.lastName = lastName;
    }

    public String getName(){
        return name;
    }

    public String getLastName(){
        return lastName;
    }
}
----

[source,java,linenums]
----
import java.util.Calendar;
class Customer extends User {
    private Calendar expirationDate;
    private User salesRepr;

    public User(){

    }

    public User(String name, String lastName,Calendar expirationDate){
        super(name,lastName);
        this.expirationDate = expirationDate;
    }

    public void setSalesRepr(User salesRepr){
        this.salesRepr = salesRepr;
    }

    public User getSalesRepr(){
        return this.salesRepr;
    }

    public void  setExpirationDate(Calendar expirationDate){
        this.expirationDate = expirationDate;
    }

    public Calendar getExpirationDate(){
        return this.expirationDate;
    }
}
----

=== Example

TODO: This example shows how to access java properties values

==== Input

TODO

[source,groovy,linenums]
----
new User("Leandro", "Shokida")
----

=== Source

TODO

[source,dataweave,linenums]
----
output application/json
---
{
    a: payload.name,
    b: payload.lastName
}
----

=== Output

TODO

[source,json,linenums]
----
{
    "a": "Leandro",
    "b": "Shokida"
}
----

=== Example

TODO: This example shows how to create an instance of a `Customer`.

TODO: NOTE: Look that it is not necessary to specify the class of inner properties as they are going to be inferred from the parent class definition


=== Source

TODO

[source,dataweave,linenums]
----
output application/json
---
{
    name: "Tomo",
    lastName: "Chibana",
    expirationDate: now(),
    salesRepr: {
        name: "Mariano",
        lastName: "de Achaval",
    }
} as Object {class: "Customer"}
----

=== Example

TODO: This example shows how to create an `java.util.ArrayList` of `User` using the generic support in the class name.

TODO: NOTE: Using generic it is not necessary to specify the class in each instance as is being taken from the generic in the list.


=== Source

TODO

[source,dataweave,linenums]
----
output application/json
---
[{
    name: "Tomo",
    lastName: "Chibana"
},
{
    name: "Ana",
    lastName: "Felissati"
},
{
    name: "Leandro",
    lastName: "Shokida"
}
] as Array {class: "java.util.ArrayList<User>"}
----


=== Writer Properties (for Java)

DataWeave accepts optional parameters that provide instructions for writing output data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `duplicateKeyAsArray` | `Boolean` | `false` | If duplicate keys are detected in
an object, the writer will change the value to an array with all those values.
Valid values are `true` or `false`.
| `writeAttributes` | `Boolean` | `false` | If a key has attributes, it will put
them as children key-value pairs of the key that contains them. The attribute
key name will start with @. Valid values are `true` or `false`.
|===

=== Custom Types (for Java)

There are a couple of custom Java types:

* `class`
* `Enum`

==== Metadata Property `class` (for Java)

Java developers use the `class` metadata key as a hint for what class needs to
be created and sent as an input. If this is not explicitly defined, DataWeave
tries to infer from the context or it assigns it the default values:

 * `java.util.HashMap` for objects
 * `java.util.ArrayList` for lists

[source,dataweave,linenums]
----
%dw 2.0
type user = Object { class: "com.anypoint.df.pojo.User"}
output application/json
---
{
  name : "Mariano",
  age : 31
} as user

----

The code above defines the type of the required input as an instance of
`com.anypoint.df.pojo.User`.

[[format_enum]]
==== Enum Custom Type (for Java)

//TODO
//MIME Type: ``

//todo
//ID: ``

In order to put an enum value in a `java.util.Map`, the DataWeave Java module
defines a custom type called `Enum`. It allows you to specify that a given
string should be handled as the name of a specified enum type. It should always
be used with the class property with the Java class name of the enum.

=== Defining a Metadata Type (for Java)

In the Transform component, you can define a Java type through the following
method:

* By providing a sample object

//START_HERE
// application/json //////////////////////////////////////////////////////////
[[format_json]]
== JSON

MIME Type: `application/json`

ID: `json`

In the JSON data format, values map one-to-one with DataWeave values.
JSON supports `String`, `Boolean`, `Number`, `Null`, `Object`, and `Array` types. DataWeave supports each of these values natively.

To understand the parsing strategies that DataWeave readers and writers can apply to this format, see xref:#dw_readers_writers[DataWeave Parsing Strategies].

[[examples_json]]
=== Examples

The following examples show uses of the JSON format.

[[example1_json]]
==== Example

This example shows how JSON input is represented in the DataWeave (dw) format.

===== Input

The following JSON object serves as the input payload to the DataWeave script.

[source,json,linenums]
----
{
  "name": "Matias",
  "age": 8,
  "male": true,
  "kids": null,
  "brothers": ["Pedro", "Sara"]
}
----

===== Source

The DataWeave script transforms the JSON input payload to the DataWeave (dw) format and MIME type.

[source,dataweave,linenums]
----
output application/dw
---
payload
----

===== Output

Notice that only the keys of the JSON input and the `application/dw` output differ. The JSON keys are surrounded by quotation marks. These DataWeave (dw)
keys do not required quotation marks. See xref:dataweave-cookbook-extract-data.adoc#valid-keys[Valid Keys] for details.

[source,dataweave,linenums]
----
{
  name: "Matias",
  age: 8,
  male: true,
  kids: null,
  brothers: [
    "Pedro",
    "Sara"
  ]
}
----

[[example2_json]]
==== Example

This example shows how to convert repeated XML elements to JSON.

Note that XML encodes collections using repeated (unbounded) elements. DataWeave represents unbounded elements by repeating the same key.

===== Input

The following XML data contains repeating child elements with the
key `name`. This XML serves as the input payload to the DataWeave script.

[source,xml,linenums]
----
<?xml version='1.0' encoding='UTF-8'?>
<friends>
  <name>Mariano</name>
  <name>Shoki</name>
  <name>Tomo</name>
  <name>Ana</name>
</friends>
----

===== Source

The following DataWeave script takes the XML input as its payload and outputs that payload in the JSON format.

The script selects the `name` elements from the XML input and uses the DataWeave `map` function to map the values of those elements into an array of objects. Each object in the array takes the form `{ name : item }`, where `item` is the value of the `name` element. The entire array serves as the value of the `friends` key.


[source,dataweave,linenums]
----
%dw 2.0
output application/json
---
friends: payload.friends.*name map (
            (item, index) -> {name: item}
         )
----

===== Output

The DataWeave script outputs the following JSON object.

[source,json,linenums]
----
{
  "friends": [
    {
      "name": "Mariano"
    },
    {
      "name": "Shoki"
    },
    {
      "name": "Tomo"
    },
    {
      "name": "Ana"
    }
  ]
}
----

=== Writer Properties (for JSON)

DataWeave accepts optional parameters that provide instructions for writing output data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
| `duplicateKeyAsArray` | `Boolean` | `false` | If duplicate keys are detected
in an object, the write will change the value to an array with all those values.
Valid values are `true` or `false`. Note that JSON language does not allow
duplicate keys with one same parent, so the duplication usually raises an
exception.
| `encoding` | `String` | `UTF-8` | The character set to use for the output.
| `indent` | `Boolean` | `true` | Indicates whether to indent the JSON code for
better readability or to compress the JSON into a single line.
Valid values are `true` or `false`.
| `skipNullOn` | `String` | None | Skips null values in the specified data
structure. By default it does not skip. Valid values are `arrays`, `objects`,
or `everywhere`. See <<skip_on_null>>.
|===

.Example: output Directive
[source,dataweave,linenums]
----
output application/json indent=false, skipNullOn="arrays"
----

=== Reader Properties (for JSON)

DataWeave accepts optional parameters that provide instructions for reading input data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `streaming` | `Boolean` | `false` | Property for streaming input. Use only if
entries are accessed sequentially. Valid values are `true` or `false`.
The input must be a top-level array. For more on streaming in DataWeave, see
<<dw_readers_writers>>.
|===

To demonstrate streaming, the following example streams a JSON file by reading
each element in an array one at a time.

.Streaming Example:
[source,xml,linenums]
----
<file:config name="File_Config" doc:name="File Config" />
<flow name="dw-streaming-jsonFlow" >
  <scheduler doc:name="Scheduler" >
    <scheduling-strategy >
      <fixed-frequency frequency="1" timeUnit="MINUTES"/>
    </scheduling-strategy>
  </scheduler>
  <file:read doc:name="Read"
     config-ref="File_Config"
     path="${app.home}/myjsonarray.json"
     outputMimeType="application/json; streaming=true"/>
  <ee:transform doc:name="Transform Message" >
    <ee:message >
      <ee:set-payload ><![CDATA[%dw 2.0
output application/json
---
payload.myJsonExample map ((element) -> {
returnedElement : element.zipcode
})]]></ee:set-payload>
    </ee:message>
  </ee:transform>
  <file:write doc:name="Write"
    path="/path/to/output/file/output.json"
    config-ref="File_Config1"/>
  <logger level="INFO" doc:name="Logger" message="#[payload]"/>
</flow>
----

* The streaming example configures the HTTP listener to stream the JSON input
by setting `outputMimeType="application/json; streaming=true"`.
In the Studio UI, you can set the *MIME Type* on the listener to `application/json`
and the *Parameters* for the MIME Type to *Key* `streaming` and *Value* `true`.
* The DataWeave script in the *Transform Message* component iterates over the
array in the input payload and selects its `zipcode` values.
* The Write operation returns a file, `output.json`, which contains the result
of the transformation.
* The Logger prints the same output payload that you see in `output.json`.

The JSON input payload looks like the following.

.JSON Input for Streaming Example (truncated):
[source,json,linenums]
----
{ "myJsonExample" : [
    {
      "name" : "Shoki",
      "zipcode": "95838"
    },
    {
      "name" : "Leandro",
      "zipcode": "95823"
    },
    {
      "name" : "Mariano",
      "zipcode": "95815"
    },
    {
      "name" : "Cristian",
      "zipcode": "95815"
    },
    {
      "name" : "Kevin",
      "zipcode": "95824"
    },
    {
      "name" : "Stanley",
      "zipcode": "95841"
    }
  ]
}
----

.Output for JSON Streaming Example:
[source,json,linenums]
----
[
  {
    "returnedElement": "95838"
  },
  {
    "returnedElement": "95823"
  },
  {
    "returnedElement": "95815"
  },
  {
    "returnedElement": "95815"
  },
  {
    "returnedElement": "95824"
  },
  {
    "returnedElement": "95841"
  }
]
----

[[skip_on_null]]
==== Skip Null On (for JSON)

You can use the `skipNullOn` writer property to omit null values from arrays, objects, or both.

When set to:

* `arrays`
+
Ignore and omit `null` values from JSON output, for example, `output application/json skipNullOn="arrays"`.
* `objects`
+
Ignore an object that has a null value. The output contains an empty object (`{}`) instead of the object with the null value, for example, `output application/json skipNullOn="objects"`.
* `everywhere`
+
Apply `skipNullOn` to arrays and objects, for example, `output application/json skipNullOn="everywhere"`.


=== Defining a Metadata Type for JSON

In the Transform component, you can define a JSON type through the following
methods:

* By providing a sample file
* By pointing to a schema file

///////////////////////////////////////////////////////////////////////////////
// application/x-ndjson ///////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
[[format_ndjson]]
== Newline Delimited JSON

MIME type: `application/x-ndjson`

ID: `ndjson`

=== Writer Properties (for ndjson)

DataWeave accepts optional parameters that provide instructions for writing output data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `writeAttributes` | `Boolean` | `false` | Valid values are `true` or `false`.
| `encoding` | `String` | |
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `skipNullOn` | `String` | | Valid values are `arrays` or `objects`.
| `deferred` | `Boolean` | `false` | Property Property for deferred output.
Valid values are `true` or `false`.
|===

=== Reader Properties (for ndjson)

DataWeave accepts optional parameters that provide instructions for reading input data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `skipInvalid` | `Boolean` | `false` | Valid values are `true` or `false`.
| `ignoreEmptyLine` | `Boolean` | `true` | Valid values are `true` or `false`.
|===

///////////////////////////////////////////////////////////////////////////////
// application/octet-stream ///////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
[[format_octet_stream]]
//DONE: Binary
== Binary

MIME Type: `application/octet-stream`

ID: `binary`

The Binary data format handles binary content, such as an image or PDF. Such text is represented as `Binary` type.

Note that Binary content is stored into RAM memory.

[[properties_avro]]
=== Configuration Properties

DataWeave supports the following configuration properties for Avro.

==== Reader Properties (for Binary)

There are no reader properties for Binary data.

==== Writer Properties (for Binary)

DataWeave accepts optional parameters that provide instructions for writing output data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
  Valid values are `true` or `false`.
|===

=== Supported MIME Types

The Binary format supports the following MIME types.

[cols="1", options="header"]
|===
| MIME Type
|`application/octet-stream`
|`application/x-binary`
|===

///////////////////////////////////////////////////////////////////////////////
// text/plain /////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
[[format_text_plain]]
== Text Plain

MIME Type: `text/plain`

ID: `text`

The Text Plain format represents text text as a `String`.

Note that DataWeave parses, encodes, and stores this format into RAM memory.

[[examples_text_plain]]
=== Example

This example shows how DataWeave represents Text Plain data.

==== Input

The Plain Text data serves as the input payload to the DataWeave script.

[source,txt,linenums]
----
This is text plain
----

==== Source

The DataWeave script transforms the Text PLain input payload to the DataWeave (dw) format and MIME type. It returns a `String`.

[source,dataweave,linenums]
----
output application/dw
---
payload
----

==== Output

Because the DataWeave (dw) output is a `String`, it is wrapped in quotation marks.

[source,dataweave,linenums]
----
"This is text plain"
----

=== Writer Properties (for text/plain)

DataWeave accepts optional parameters that provide instructions for writing output data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `encoding` | `String` | None | Encoding for the writer to use.
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
  Valid values are `true` or `false`.
|===

// text/x-java-properties //////////////////////////////////////////////
[[format_x_java_properties]]
== Text Java Properties

MIME Type: `text/x-java-properties`

ID: `properties`

The Text Java Properties format parses any Java properties file. This format represents simple key-value pairs. DataWeave represents these pairs as an `Object` with values of the `String` type.

[[examples_java_properties]]
=== Example

This example shows how DataWeave represents a properties file.

==== Input

The following `text/x-java-properties` data is from a properties file. The file contains key-value pairs that provide host and port values. This content serves as the input payload to the DataWeaave script.

[source,properties,linenums]
----
host=localhost
port=1234
----

==== Source

The DataWeave script transforms the `text/x-java-properties` input payload to the DataWeave (dw) format and MIME type. It returns a `String`.

[source,dataweave,linenums]
----
%dw 2.0
output application/dw
---
payload
----

==== Output

The output is an object in the DataWeave (dw) format. The object contains a collection of key-value pairs that match the `text/x-java-properties` input. Notice that the output wraps the values in quotation marks.

[source,dataweave,linenums]
----
{
    host: "localhost",
    port: "1234"
}
----

=== Writer Properties (for Text Java Properties)

DataWeave accepts optional parameters that provide instructions for writing output data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `encoding` | `String` | None | Encoding for the writer to use.
| `bufferSize` | `Number` | `8192` |Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
  Valid values are `true` or `false`.
|===

// application/xml //////////////////////////////////////////////////////////
[[format_xml]]
== XML

MIME Type: `application/xml`

ID: `xml`

The XML data structure is mapped to DataWeave objects that can contain other
`Object`, `String` or `Null` values to their keys. XML uses unbounded elements to represent collections, which are mapped to repeated keys in DataWeave objects. In addition, DataWeave natively supports `namespaces`, `CData` an `xsi:types`.

To understand the parsing strategies that DataWeave readers and writers can apply to this format, see xref:#dw_readers_writers[DataWeave Parsing Strategies].

[[custom_types_xml]]
=== Custom Types

TODO

==== CDATA

`CData` is a custom data type for XML that is used to identify a `CDATA` XML
block. It can tell the writer to wrap the content inside `CDATA` or to check
if the input string arrives inside a `CDATA` block. `CData` inherits from the
type `String`.

TODO: THERE'S A ANOTHER, SEPARATE SECTION ON THIS IN THE DOC.

=== Streaming

Starting in Mule version 4.3.0, XML supports streaming. You need to specify the following reader property to activate streaming:

* `collectionPath`:
+
Specifies a path expression the identifies the location of the elements to stream.

When streaming, the XML parser can start processing content without having all the XML content.

[[examples_xml]]
=== Examples

The following examples show uses of the XML format.

[[example1_xml]]
===== Example

//TODO: I DON'T SEE THOSE PROPS SET IN THE SOURCE!
This example shows how to set up XML streaming, which requires you to specify the following reader properties:

* `streaming=true`
* `collectionPath="root.repeated"`

The `collectionPath` setting selects to the elements to stream. that are going to be stream that are al the `user`

===== Input

TODO

[source,xml,linenums]
----
<root>
    <text>
        Text
    </text>
    <repeated>
        <user>
            <name>Mariano</name>
            <lastName>de Achaval</lastName>
            <age>36</age>
        </user>
        <user>
            <lastName>Shokida</lastName>
            <name>Leandro</name>
            <age>30</age>
        </user>
        <user>
            <age>29</age>
            <name>Ana</name>
            <lastName>Felissati</lastName>

        </user>
        <user>
            <age>29</age>
            <lastName>Chibana</lastName>
            <name>Christian</name>
        </user>

    </repeated>
</root>
----

===== Source

The DataWeave script transforms the XML input payload to the DataWeave (dw) format and MIME type.

TODO: MISSING STREAMING PROPERTIES MENTIONED IN THE INTRO.
TODO: ALSO NEED TO ADD INTRO.

[source,dataweave,linenums]
----
%dw 2.0
output application/dw
---
payload.root.repeated.*user map {
    n: $.name,
    l: $.lastName,
    a: $.age
}
----

===== Output

[source,json,linenums]
----
[
  {
    "n": "Mariano",
    "l": "de Achaval",
    "a": "36"
  },
  {
    "n": "Leandro",
    "l": "Shokida",
    "a": "30"
  },
  {
    "n": "Ana",
    "l": "Felissati",
    "a": "29"
  },
  {
    "n": "Christian",
    "l": "Chibana",
    "a": "29"
  }
]
----

[[example2_xml]]
===== Example: Null or Empty String in XML

Because there is no standard way to represent a null value in XML, the reader maps to `null` when the `nil` attribute is set to `true`.

This example maps `nil` to `null`.

===== Input

TODO

[source,xml,linenums]
----
<book  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <author xsi:nil="true"/>
</book>
----

===== Source

TODO

[source,dataweave,linenums]
----
output application/json
---
payload
----

===== Output

TODO

[source,json,linenums]
----
{
  "book": {
    "author": null
  }
}
----

[[example2_xml]]
===== Example: Setting the `nullValueOn` Property

Setting the reader property `nullValueOn` to `blank` or `empty`. By default is set to `blank`.

This example shows how the property `title` and `author` are mapped to null as `nullValueOn` is set to `blank`.

===== Input

TODO

[source,xml,linenums]
----
<book>
    <author></author>
    <title>

</title>
</book>
----

===== Source

TODO

[source,dataweave,linenums]
----
output application/json
---
payload
----

===== Output

TODO

[source,json,linenums]
----
{
  "book": {
    "author": null,
    "title": null
  }
}
----

[[example3_xml]]
===== Example

TODO

This example shows how the property `title` is mapped to a `String` and `author` is mapped to null as `nullValueOn` is set to `empty`.

===== Input

TODO

[source,xml,linenums]
----
<book>
    <author></author>
    <title>

</title>
</book>
----

===== Source

TODO

[source,dataweave,linenums]
----
output application/json
---
payload
----

===== Output

TODO

[source,json,linenums]
----
{
  "book": {
    "author": null,
    "title": "\n\n"
  }
}
----

[[example4_xml]]
===== Example

TODO - When the reader property `nullValueOn` is set to `blank` or is set `empty`. By default is set to `blank`

TODO

This example shows how the property `title` and `author` are mapped to  String` as `nullValueOn` is set to `none`.

==== Input

TODO

[source,xml,linenums]
----
<book>
    <author></author>
    <title>

</title>
</book>
----

===== Source

TODO

[source,dataweave,linenums]
----
output application/json
---
payload
----

===== Output

TODO

[source,json,linenums]
----
{
  "book": {
    "author": "",
    "title": "\n\n"
  }
}
----

[[example5_xml]]
===== Example: TODO

TODO
This example shows how a simple XML is being mapped to DataWeave canonical representation.

===== Input

TODO

[source,xml,linenums]
----
<users>
  <company>MuleSoft</company>
  <user name="Leandro" lastName="Shokida"/>
  <user name="Mariano" lastName="Achaval"/>
</users>
----

===== Source

The DataWeave script transforms the XML input payload to the DataWeave (dw) format and MIME type.

[source,dataweave,linenums]
----
output application/dw
---
payload
----

===== Output

TODO

[source,dataweave,linenums]
----
{
  users: {
    company: "MuleSoft",
    user @(name: "Leandro",lastName: "Shokida"): "",
    user @(name: "Mariano",lastName: "Achaval"): ""
  }
}
----

[[example6_xml]]
=== Example

This example how namespaces are being mapped to DataWeave canonical representation.

==== Input

TODO

[source,xml,linenums]
----
<root>
    <h:table xmlns:h="http://www.w3.org/TR/html4/">
      <h:tr>
        <h:td>Apples</h:td>
        <h:td>Bananas</h:td>
      </h:tr>
    </h:table>

    <f:table xmlns:f="https://www.w3schools.com/furniture">
      <f:name>African Coffee Table</f:name>
      <f:width>80</f:width>
      <f:length>120</f:length>
    </f:table>
</root>
----

===== Source

The DataWeave script transforms the XML input payload to the DataWeave (dw) format and MIME type.

[source,dataweave,linenums]
----
output application/dw
---
payload
----

===== Output

TODO

[source,dataweave,linenums]
----
ns h http://www.w3.org/TR/html4/
ns f https://www.w3schools.com/furniture
---
{
  root: {
      h#table: {
        h#tr: {
          h#td: "Apples",
          h#td: "Bananas"
        }
      },
      f#table: {
        f#name: "African Coffee Table",
        f#width: "80",
        f#length: "120"
      }
  }
}
----

[[example7_xml]]
===== Example

TODO

This example shows how `CData` type is being used to create CDATA xml element.

===== Source

TODO

[source,dataweave,linenums]
----
%dw 2.0
output application/xml
---
{
    test: "A text <a>" as CData
}
----

===== Output

TODO

[source,xml,linenums]
----
<?xml version='1.0' encoding='UTF-8'?>
<test><![CDATA[A text <a>]]></test>
----

[[example8_xml]]
===== Example

TODO
This example shows how `CData` to check if a String value is a CDATA

===== Input

TODO

[source,xml,linenums]
----
<?xml version='1.0' encoding='UTF-8'?>
<test><![CDATA[A text <a>]]></test>
----

===== Source

TODO

[source,dataweave,linenums]
----
%dw 2.0
output application/json
---
{
    test: payload.test is CDATA
}
----

===== Output

TODO

[source,json,linenums]
----
{
    "test": true
}
----

[[example9_xml]]
===== Example

TODO

This example shows how `inlineCloseOn` with value none use different tag to close the xml.

===== Source

TODO

[source,dataweave,linenums]
----
output application/xml inlineCloseOn="none"
---
{
  someXml: {
    parentElement: {
      emptyElement1: null,
      emptyElement2: null,
      emptyElement3: null
    }
  }
}
----

===== Output

TODO

[source,xml,linenums]
----
<?xml version='1.0' encoding='UTF-8'?>
<someXml>
  <parentElement>
    <emptyElement1></emptyElement1>
    <emptyElement2></emptyElement2>
    <emptyElement3></emptyElement3>
  </parentElement>
</someXml>
----

[[example10_xml]]
===== Example

TODO
Xml encodes collections using repeated elements (UNBOUNDED elements) this is represented in DataWeave by repeating the same key.
This example shows how to convert a JSON Array into a repeated xml element.

===== Input

TODO

[source,json,linenums]
----
{
  "friends": [
    {"name": "Mariano"},
    {"name": "Shoki"},
    {"name": "Tomo"},
    {"name": "Ana"}
  ]
}
----

===== Source

TODO

[source,dataweave,linenums]
----
%dw 2.0
output application/xml
---
friends: {
    (payload.friends)
}
----

===== Output

TODO

[source,xml,linenums]
----
<?xml version='1.0' encoding='UTF-8'?>
<friends>
  <name>Mariano</name>
  <name>Shoki</name>
  <name>Tomo</name>
  <name>Ana</name>
</friends>
----

//NOT NEW
[[example11_xml]]
===== Example
TODO

===== Input
TODO

[source,xml,linenums]
----
<users>
  <company>MuleSoft</company>
  <user name="Leandro" lastName="Shokida"/>
  <user name="Mariano" lastName="Achaval"/>
</users>
----

===== Output
TODO
.DataWeave Script:
[source,dataweave,linenums]
----
{
  users: {
    company: "MuleSoft",
    user @(name: "Leandro",lastName: "Shokida"): "",
    user @(name: "Mariano",lastName: "Achaval"): ""
  }
}
----

//NOT NEW
[[example12_xml]]
===== Example

TODO
This example shows how the property `title` is mapped to a `String` and `author` is mapped to null as `nullValueOn` is set to `empty`.

===== Input

TODO
[source,xml,linenums]
----
<book>
    <author></author>
    <title>

</title>
</book>
----

===== Source

TODO
[source,dataweave,linenums]
----
output application/json
---
payload
----

===== Output

[source,json,linenums]
----
{
  "book": {
    "author": null,
    "title": "\n\n"
  }
}
----

//NOT NEW
[[example12_xml]]
===== Example: Use of the inlineCloseOn Parameter

The `inlineCloseOn` writer property indicates whether to output empty elements with the default structure (`none`) or to close the element by setting the value to `empty`.

.Empty Elements That are Unlosed:
[source,xml,linenums]
----
<someXml>
  <parentElement>
    <emptyElement1></emptyElement1>
    <emptyElement2></emptyElement2>
    <emptyElement3></emptyElement3>
  </parentElement>
</someXml>
----

.Empty Elements That are Closed:
[source,xml,linenums]
----
<payload>
  <someXml>
    <parentElement>
      <emptyElement1/>
      <emptyElement2/>
      <emptyElement3/>
    </parentElement>
  </someXml>
</payload>
----

See also, xref:dataweave-cookbook-output-self-closing-xml-tags.adoc[Example: Outputting Self-closing XML Tags].


[[properties_xml]]
=== Configuration Properties

DataWeave supports the following configuration properties for XML.

==== Reader Properties (for XML)

DataWeave accepts optional parameters that provide instructions for reading input data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `maxEntityCount` | `Number` | `1` | The maximum number of entity expansions.
The limit is in place to avoid Billion Laughs attacks.
| `indexedReader` | `Boolean` | `true` | If the indexed XML reader should be
used when the threshold is reached. Valid values are `true` or `false`.
//| `maxAttributeSize` | `Number` | `-1` | Sets the maximum number of characters
accepted in an XML attribute. Available since Mule 4.2.1.
| `nullValueOn` | `String` | `blank` | If a tag with empty or blank text should
be read as null. Valid values are `empty`, `none`, or `blank`.
| `externalEntities` | `Boolean` | `false` | Indicates whether external entities
should be processed or not. By default this is disabled to avoid XXE attacks.
Valid values are `true` or `false`.
|===

=== Writer Properties (for XML)

DataWeave accepts optional parameters that provide instructions for writing output data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `encoding` | `String` | None |Encoding for the writer to use.
| `deferred` | `Boolean` | `false` |Property for deferred output.
  Valid values are `true` or `false`.
//| `escapeCR` | `Boolean` | `false` | Whether to escape a carriage return (CR).
Valid values are `true` or `false`. Available since Mule 4.2.1.
| `indent` | `Boolean` | `true` |Indicates whether to indent the output.
Valid values are `true` or `false`.
| `inlineCloseOn` | `String` | `empty` |When the writer should use inline close
tag. Valid values are `empty` or `none`.
| `onInvalidChar` | `String` | None |Valid values are `base64`, `ignore`, or `none`.
| `writeNilOnNull` | `Boolean` | `false` |Whether to write a nil attribute when
the value is null. Valid values are `true` or `false`.
| `skipNullOn` | `String` | None |Skips null values in the specified data
structure. By default it does not skip. Valid values are `elements`, `attributes`,
or `everywhere`. See <<skip_on_null_xml>>
| `writeDeclaration` | `Boolean` | `true` |Indicates whether to write the XML
header declaration. Valid values are `true` or `false`.
|===

.Example: output Directive
[source,dataweave,linenums]
----
output application/xml indent=false, skipNullOn="attributes"
----

[[skip_on_null_xml]]
==== Skip Null On (for XML)

You can specify whether your transform generates an outbound message that
contains fields with "null" values, or if these fields are ignored entirely.
This can be set through an attribute in the output directive named `skipNullOn`,
which can be set to three different values:
`elements`, `attributes`, or `everywhere`.

When set to:

* `elements`: A key:value pair with a null value is ignored.
* `attributes`: An XML attribute with a null value is skipped.
* `everywhere`: Apply this rule to both elements and attributes.

=== Defining a Metadata Type (for XML)

In the Transform component, you can define a XML type through the following
methods:

* By providing a sample file
* By pointing to a schema file

[[format_cdata]]
=== CData Custom Type (for XML)

MIME Type: `application/xml`

ID: `xml`

`CData` is a custom data type for XML that is used to identify a CDATA XML
block. It can tell the writer to wrap the content inside CDATA or to check
if the input string arrives inside a CDATA block. `CData` inherits from the
type `String`.

.DataWeave Script::
[source,dataweave,linenums]
----
%dw 2.0
output application/xml
---
{
  users:
  {
    user : "Mariano" as CData,
    age : 31 as CData
  }
}
----

.Output:
[source,xml,linenums]
----
<?xml version="1.0" encoding="UTF-8"?>
<users>
  <user><![CDATA[Mariano]]></user>
  <age><![CDATA[31]]></age>
</users>
----

///////////////////////////////////////////////////////////////////////////
// application/x-www-form-urlencoded //////////////////////////////////
///////////////////////////////////////////////////////////////////////////
//TODO: SKIPPED TO WORK ON XML
[[format_url_encoded]]
== URL Encoded

MIME Type: `application/x-www-form-urlencoded`

ID: `urlencoded`

URL encoded data represents a collection key-value pairs. DataWeave represents these values as an `Object` in which each value is of the type `String`.

[[examples_url_encoded]]
=== Examples

The following examples show uses of the URL Encoded format.

[[example1_url_encoded]]
==== Example

This example shows how DataWeave represents simple URL encoded data.

==== Input

The URL encoded data serves as input payload to the DataWeave script.

[source,form-urlencoded,linenums]
----
name=Mariano&lastName=de+Achaval
----

===== Source

The DataWeave script transforms the URL encoded input payload to the DataWeave (dw) format and MIME type.

[source,dataweave,linenums]
----
%dw 2.0
output application/dw
---
payload
----

===== Output

The output is a DataWeave object that contains a collection of key-value pairs from the input.

[source,dataweave,linenums]
----
{
    "name": "Mariano",
    "lastName": "de Achaval"
}
----

=== Example

This example shows how to generate a form-urlencoded from a simple script

==== Input

[source,json,linenums]
----
{
  "name": "Mariano"
}
----

===== Source

[source,dataweave,linenums]
----
%dw 2.0
output urlencoded
---
{
    name: payload.name,
    age: 37
}
----

===== Output

[source,form-urlencoded,linenums]
----
name=Mariano&age=37
----

//NOT NEW
===== Example

//TODO: PROVIDE DESCRIPTION

===== Source

The following DataWeave script produces the data URL encoded output.

.DataWeave Object
[source,dataweave,linenums]
----
%dw 2.0
output application/x-www-form-urlencoded
---
{
  "key" : "value",
  "key 1": "@here",
  "key" : "other value",
  "key 2%": null
}
----

===== Output

//TODO: PROVIDE DESCRIPTION

[source,text,linenums]
----
key=value&key+1=%40here&key=other+value&key+2%25
----

//NOT NEW
===== Example

You can also read in the <<raw_data>> above as input to the DataWeave script
in the next example to return `value@here` as the result.

===== Source

//TODO: PROVIDE DESCRIPTION

[source,dataweave,linenums]
----
%dw 2.0
var myData = read('key=value&key+1=%40here&key=other+value&key+2%25', 'application/x-www-form-urlencoded')
output text/plain
---
myData.*key[0] ++ myData.'key 1'
----

===== Output

//TODO: PROVIDE DESCRIPTION

[source,text,linenums]
----
value@here
----

//NOT NEW BUT REORG'D
===== Example

The following examples show `output` directives with and without configuration properties.

* `output application/x-www-form-urlencoded`
* `output application/x-www-form-urlencoded encoding="UTF-8", bufferSize="500"`

Note that in the DataWeave `write` function, you can also pass the property as
an optional parameter. The scope of the property is limited to the DataWeave
script where you call the function.

[[properties_urlencoded]]
=== Configuration Properties

DataWeave supports the following configuration properties for `application/x-www-form-urlencoded`.

==== Reader Properties (for URL Encoding)

There are no reader properties for URL encoded data.

==== Writer (for URL Encoded Data)

DataWeave accepts optional parameters that provide instructions for writing output data.

// TODO: ASK SHOKI ABOUT KB VS BYTES
[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `encoding` | `String` | None | Encoding for the writer to use.
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
  Valid values are `true` or `false`.
|===

///////////////////////////////////////////////////////////////////////////
// application/yaml ///////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////
[[format_yaml]]
== YAML

MIME Type: `application/yaml`

ID: `yaml`

Values in the JSON data format map one-to-one with DataWeave values.
DataWeave natively supports all of the following JSON types:

* String
* Boolean
* Number
* Nil
* Mapping
* Sequences

[[examples_yaml]]
=== Example

This example shows how DataWeave represents YAML values.

==== Input

The following YAML snippet serves as the input payload for the DataWeave script
in this example.

[source,yaml,linenums]
----
american:
  - Boston Red Sox
  - Detroit Tigers
  - New York Yankees
national:
  - New York Mets
  - Chicago Cubs
  - Atlanta Braves
----

==== Source

The DataWeave script transforms the YAML encoded input payload to the DataWeave (dw) format and MIME type.

[source,dataweave,linenums]
----
%dw 2.0
output application/dw
---
payload
----

==== Output

The following output shows how the YAML input is represented in the DataWeave (`dw`) format.

[source,dataweave,linenums]
----
{
  "american": [
    "Boston Red Sox",
    "Detroit Tigers",
    "New York Yankees"
  ],
  "national": [
    "New York Mets",
    "Chicago Cubs",
    "Atlanta Braves"
  ]
}
----

[[properties_yaml]]
=== Configuration Properties (for YAML)

DataWeave supports the following configuration properties.

==== Writer Properties (for YAML)

DataWeave accepts optional parameters that provide instructions for writing output data.

[cols="1,1,1,3a", options="header"]
|===
| Parameter | Type | Default | Description
| `encoding` | `String` | `UTF-8` | Encoding for the writer to use.
| `bufferSize` | `Number` | `8192` | Size of the buffer writer.
| `deferred` | `Boolean` | `false` | Property for deferred output.
  Valid values are `true` or `false`.
| `skipNullOn` | `String` | None | Skips `null` values in the specified data
structure. Valid values are `arrays`, `objects`, or `everywhere`. The writer does not skip such values unless you set the property.
|===

==== Reader Properties (for YAML)

DataWeave accepts optional parameters that provide instructions for reading input data.

[cols="1,1,1,3a", options="header"]
|===
|Parameter |Type |Default|Description
| `maxEntityCount` | `Number` | `1` | The maximum number of entity expansions. The limit is provided to avoid the billion laughs attack, a denial-of-service attack.
|===

=== Supported MIME Types (for YAML)

DataWeave supports the following MIME types.

[cols="1", options="header"]
|===
| MIME Type
|`application/yaml`
|`text/yaml`
|`application/x-yaml`
|`text/x-yaml`
|===

== See Also

xref:studio::transform-message-component-concept-studio.adoc[Transform Message Component]

xref:dataweave-flat-file-schemas.adoc[Flat File Schemas]
